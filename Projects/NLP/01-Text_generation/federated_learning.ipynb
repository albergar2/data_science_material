{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Federated Learning - Text Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Course: Machine Learning Projects with TensorFlow 2.0 by Vlad Sebastian Ionescu*\n",
    "\n",
    "*Tutorial: https://www.tensorflow.org/federated/tutorials/federated_learning_for_text_generation#load_and_preprocess_the_federated_shakespeare_data*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Federated Learning means training on a distributed way and combining the results on a centralized server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "import tensorflow_federated as tff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'Hello, World!'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import collections\n",
    "import functools\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "nest_asyncio.apply()\n",
    "tf.compat.v1.enable_v2_behavior()\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "tff.federated_computation(lambda: 'Hello, World!')()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A fixed vocabularly of ASCII chars that occur in the works of Shakespeare and Dickens:\n",
    "vocab = list('dhlptx@DHLPTX $(,048cgkoswCGKOSW[_#\\'/37;?bfjnrvzBFJNRVZ\"&*.26:\\naeimquyAEIMQUY]!%)-159\\r')\n",
    "\n",
    "# Creating a mapping from unique characters to indices\n",
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = tff.simulation.datasets.shakespeare.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b\"Live regist'red upon our brazen tombs,\\nAnd then grace us in the disgrace of death;\\nWhen, spite of cormorant devouring Time,\\nTh' endeavour of this present breath may buy\\nThat honour which shall bate his scythe's keen edge,\\nAnd make us heirs of all eternity.\\nTherefore, brave conquerors- for so you are\\nThat war against your own affections\\nAnd the huge army of the world's desires-\\nOur late edict shall strongly stand in force:\\nNavarre shall be the wonder of the world;\\nOur court shall be a little Academe,\\nStill and contemplative in living art.\\nYou three, Berowne, Dumain, and Longaville,\\nHave sworn for three years' term to live with me\\nMy fellow-scholars, and to keep those statutes\\nThat are recorded in this schedule here.\\nYour oaths are pass'd; and now subscribe your names,\\nThat his own hand may strike his honour down\\nThat violates the smallest branch herein.\\nIf you are arm'd to do as sworn to do,\\nSubscribe to your deep oaths, and keep it too.\\nYour oath is pass'd to pass away from these.\", shape=(), dtype=string)\n",
      "tf.Tensor(b'this?\\nDid you hear the proclamation?', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "# Here the play is \"The Tragedy of King Lear\" and the character is \"King\".\n",
    "raw_example_dataset = train_data.create_tf_dataset_for_client(\n",
    "    'THE_TRAGEDY_OF_KING_LEAR_KING')\n",
    "# To allow for future extensions, each entry x\n",
    "# is an OrderedDict with a single key 'snippets' which contains the text.\n",
    "for x in raw_example_dataset.take(2):\n",
    "  print(x['snippets'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input pre-processing parameters\n",
    "SEQ_LENGTH = 100\n",
    "BATCH_SIZE = 8\n",
    "BUFFER_SIZE = 100  # For dataset shuffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a lookup table to map string chars to indexes,\n",
    "# using the vocab loaded above:\n",
    "table = tf.lookup.StaticHashTable(\n",
    "    tf.lookup.KeyValueTensorInitializer(\n",
    "        keys=vocab, values=tf.constant(list(range(len(vocab))),\n",
    "                                       dtype=tf.int64)),\n",
    "    default_value=0)\n",
    "\n",
    "\n",
    "def to_ids(x):\n",
    "    s = tf.reshape(x['snippets'], shape=[1])\n",
    "    chars = tf.strings.bytes_split(s).values\n",
    "    ids = table.lookup(chars)\n",
    "    return ids\n",
    "\n",
    "\n",
    "def split_input_target(chunk):\n",
    "    input_text = tf.map_fn(lambda x: x[:-1], chunk)\n",
    "    target_text = tf.map_fn(lambda x: x[1:], chunk)\n",
    "    return (input_text, target_text)\n",
    "\n",
    "\n",
    "def preprocess(dataset):\n",
    "    return (\n",
    "      # Map ASCII chars to int64 indexes using the vocab\n",
    "      dataset.map(to_ids)\n",
    "      # Split into individual chars\n",
    "      .unbatch()\n",
    "      # Form example sequences of SEQ_LENGTH +1\n",
    "      .batch(SEQ_LENGTH + 1, drop_remainder=True)\n",
    "      # Shuffle and form minibatches\n",
    "      .shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "      # And finally split into (input, target) tuples,\n",
    "      # each of length SEQ_LENGTH.\n",
    "      .map(split_input_target))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(TensorSpec(shape=(8, 100), dtype=tf.int64, name=None), TensorSpec(shape=(8, 100), dtype=tf.int64, name=None))\n"
     ]
    }
   ],
   "source": [
    "example_dataset = preprocess(raw_example_dataset)\n",
    "print(example_dataset.element_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(batch_size):\n",
    "    urls = {\n",
    "        1: 'https://storage.googleapis.com/tff-models-public/dickens_rnn.batch1.kerasmodel',\n",
    "        8: 'https://storage.googleapis.com/tff-models-public/dickens_rnn.batch8.kerasmodel'}\n",
    "    assert batch_size in urls, 'batch_size must be in ' + str(urls.keys())\n",
    "    url = urls[batch_size]\n",
    "    local_file = tf.keras.utils.get_file(os.path.basename(url), origin=url)  \n",
    "    return tf.keras.models.load_model(local_file, compile=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string):\n",
    "    \n",
    "    # Number of characters to generate\n",
    "    num_generate = 1000\n",
    "    \n",
    "    # Coverting the start_string to numbers (vectorizing)\n",
    "    input_eval = [char2idx[s] for s in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "    \n",
    "    text_generated = []\n",
    "    # Low temperature --> more predictable text\n",
    "    # High temperature --> more surprising text\n",
    "    temperature = 1.0\n",
    "    \n",
    "    model.reset_states()\n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "        \n",
    "        # Using a categorical distribution to predict the char returned by the model\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
    "        \n",
    "        # We pass the predicted char to the model along with the previous hidden state\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "        \n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "        \n",
    "    return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What of TensorFlow Federated, you ask? S.\n",
      "\n",
      "\"So frightfully for fave to come or not trace, and touch the crime as\n",
      "got up interest; and what we supportable I had, and he cannot\n",
      "sure not that silence. It had a ridier wn the village, up of Saint Antoine, and\n",
      "to be the object that could not conseng to the lighting up into the dess, \"and how can you say he was,\n",
      "it seemed after you, I am glided whom the\n",
      "straite had made his three last shining upon it--\"I must foin. Proof should keep in Suspicion\n",
      " thoughtfully and as me! but only to light I am highly\n",
      "for that Defarges, and a few Frame ago. From the gendral, showed him greatly.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "IX. Two\n",
      "\n",
      "magn from our mut of the Project Gutenberg time when he had wandered away uarted in his sinister family; it may be madness me.\n",
      "Le very good what midday, and brought him his hand her\n",
      "lips to himself as he wore, whisper itself to the coachman, and the distant streets, Mr.\n",
      "Lorry's eyes gradually sound, like a dream, Charles Darnay swoonspend out the air, lagagain, on\n",
      "enforming me\n"
     ]
    }
   ],
   "source": [
    "# Text generation requires a batch_size=1 model.\n",
    "keras_model_batch1 = load_model(batch_size=1)\n",
    "print(generate_text(keras_model_batch1, 'What of TensorFlow Federated, you ask? '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8  # The training and eval batch size for the rest of this tutorial.\n",
    "keras_model = load_model(batch_size=BATCH_SIZE)\n",
    "keras_model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tff_model():\n",
    "    # TFF uses an `input_spec` so it knows the types and shapes\n",
    "    # that your model expects.\n",
    "    input_spec = example_dataset.element_spec\n",
    "    keras_model_clone = tf.keras.models.clone_model(keras_model)\n",
    "    \n",
    "    return tff.learning.from_keras_model(\n",
    "        keras_model_clone,\n",
    "        input_spec=input_spec,\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/albertogarcia/opt/anaconda3/envs/ml_projects/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:1666: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/albertogarcia/opt/anaconda3/envs/ml_projects/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:1666: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['gru_1/gru_cell/kernel:0', 'gru_1/gru_cell/recurrent_kernel:0', 'gru_1/gru_cell/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['gru_1/gru_cell/kernel:0', 'gru_1/gru_cell/recurrent_kernel:0', 'gru_1/gru_cell/bias:0'] when minimizing the loss.\n"
     ]
    }
   ],
   "source": [
    "# This command builds all the TensorFlow graphs and serializes them: \n",
    "fed_avg = tff.learning.build_federated_averaging_process(\n",
    "    model_fn=create_tff_model,\n",
    "    client_optimizer_fn=lambda: tf.keras.optimizers.SGD(lr=0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=<loss=4.401425838470459>\n",
      "loss=<loss=4.270419120788574>\n",
      "loss=<loss=4.158207893371582>\n",
      "loss=<loss=4.049384117126465>\n",
      "loss=<loss=3.96478271484375>\n"
     ]
    }
   ],
   "source": [
    "nest_asyncio.apply()\n",
    "NUM_ROUNDS = 5\n",
    "state = fed_avg.initialize()\n",
    "for _ in range(NUM_ROUNDS):\n",
    "    state, metrics = fed_avg.next(state, [example_dataset.take(5)])\n",
    "    print(f'loss={metrics.train}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
