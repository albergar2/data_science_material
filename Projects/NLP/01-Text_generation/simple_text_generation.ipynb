{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Text Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Course: Machine Learning Projects with TensorFlow 2.0 by Vlad Sebastian Ionescu*\n",
    "\n",
    "*Data: https://www.kaggle.com/kazanova/sentiment140*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>351005</th>\n",
       "      <td>0</td>\n",
       "      <td>2018151132</td>\n",
       "      <td>Wed Jun 03 09:31:03 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>loahriot</td>\n",
       "      <td>it gets harder each time he leaves. I haven't...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>906944</th>\n",
       "      <td>4</td>\n",
       "      <td>1695583037</td>\n",
       "      <td>Mon May 04 06:29:11 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>JaneHungOz</td>\n",
       "      <td>@1critic Yep, ah, damn, I don't wanna leave my...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1326113</th>\n",
       "      <td>4</td>\n",
       "      <td>2015220665</td>\n",
       "      <td>Wed Jun 03 04:03:25 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>tenyentegebs</td>\n",
       "      <td>be my fellawor.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1590265</th>\n",
       "      <td>4</td>\n",
       "      <td>2191371977</td>\n",
       "      <td>Tue Jun 16 05:08:08 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Missbabyp</td>\n",
       "      <td>omg the test went great! i'm happy, i'm very h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1086331</th>\n",
       "      <td>4</td>\n",
       "      <td>1969298360</td>\n",
       "      <td>Fri May 29 23:36:06 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>rachelteamICO</td>\n",
       "      <td>Testing that my laptop works from home  Yes it...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0           1                             2         3              4  \\\n",
       "351005   0  2018151132  Wed Jun 03 09:31:03 PDT 2009  NO_QUERY       loahriot   \n",
       "906944   4  1695583037  Mon May 04 06:29:11 PDT 2009  NO_QUERY     JaneHungOz   \n",
       "1326113  4  2015220665  Wed Jun 03 04:03:25 PDT 2009  NO_QUERY   tenyentegebs   \n",
       "1590265  4  2191371977  Tue Jun 16 05:08:08 PDT 2009  NO_QUERY      Missbabyp   \n",
       "1086331  4  1969298360  Fri May 29 23:36:06 PDT 2009  NO_QUERY  rachelteamICO   \n",
       "\n",
       "                                                         5  \n",
       "351005    it gets harder each time he leaves. I haven't...  \n",
       "906944   @1critic Yep, ah, damn, I don't wanna leave my...  \n",
       "1326113                                   be my fellawor.   \n",
       "1590265  omg the test went great! i'm happy, i'm very h...  \n",
       "1086331  Testing that my laptop works from home  Yes it...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('data/tweets.csv', encoding='latin-1', header=None)\n",
    "data = data.sample(frac=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.7*len(data))\n",
    "features = data[5]\n",
    "target = data[0]\n",
    "X_train, X_test = features.values[:train_size], features.values[train_size:]\n",
    "y_train, y_test = target.values[:train_size], target.values[train_size:]\n",
    "\n",
    "y_train[y_train==2] = 1\n",
    "y_train[y_train==4] = 2\n",
    "\n",
    "y_test[y_test==2] = 1\n",
    "y_test[y_test==4] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(max_features=100)\n",
    "X_train_num = count_vectorizer.fit_transform(X_train).toarray()\n",
    "X_test_num = count_vectorizer.transform(X_test).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@ajam247 ty! I haven't been there since we went in '97! I'll ride our ride, Vortex, for ya! \n",
      "[0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "instance = 14\n",
    "print(X_train[instance])\n",
    "print(X_train_num[instance])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Text Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 2 0 ... 0 2 0]\n",
      "Epoch 1/2\n",
      "1575/1575 - 3s - loss: 0.6390 - accuracy: 0.6417 - val_loss: 0.6189 - val_accuracy: 0.6590\n",
      "Epoch 2/2\n",
      "1575/1575 - 3s - loss: 0.6184 - accuracy: 0.6546 - val_loss: 0.6164 - val_accuracy: 0.6619\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fe55ec76450>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(256, activation=tf.nn.leaky_relu),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(3)\n",
    "])\n",
    "\n",
    "adam_optimizer = tf.keras.optimizers.Adam()\n",
    "model.compile(\n",
    "    optimizer=adam_optimizer,\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(y_train)\n",
    "\n",
    "model.fit(X_train_num,\n",
    "          y_train,\n",
    "          batch_size=64,\n",
    "          epochs=2,\n",
    "          validation_split=0.1,\n",
    "          verbose=2\n",
    "          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Text Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" it gets harder each time he leaves. I haven't been able to get a goodnight sleep since then. i miss him @1critic Yep, ah, damn, I don't wanna leave my warm doona to get a hot beverage...I guess I'll just have to make do w/o it  be my fellawor.  omg the test went great! i'm happy, i'm very happy!! a\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = ' '.join(data[5])\n",
    "text[:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "193 unique characters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 1, 71, 82, ..., 19, 19, 19])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a unique set of characters (vocabulary)\n",
    "vocab = sorted(set(text))\n",
    "print(f'{len(vocab)} unique characters')\n",
    "\n",
    "# Map unique characters to an index and the opposite\n",
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "text_as_int = np.array([char2idx[c] for c in text])\n",
    "text_as_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "i\n",
      "t\n",
      " \n",
      "g\n"
     ]
    }
   ],
   "source": [
    "# Maximum length in characters for a sentence\n",
    "seq_length = 64\n",
    "examples_per_epoch = len(text) // (seq_length+1)\n",
    "\n",
    "# Creating training examples\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "for i in char_dataset.take(5):\n",
    "    print(idx2char[i.numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" it gets harder each time he leaves. I haven't been able to get a\"\n",
      "' goodnight sleep since then. i miss him @1critic Yep, ah, damn, I'\n",
      "\" don't wanna leave my warm doona to get a hot beverage...I guess \"\n",
      "\"I'll just have to make do w/o it  be my fellawor.  omg the test w\"\n",
      "\"ent great! i'm happy, i'm very happy!! and i finally have wireles\"\n"
     ]
    }
   ],
   "source": [
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for item in sequences.take(5):\n",
    "    print(repr(''.join(idx2char[item.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data:  \" it gets harder each time he leaves. I haven't been able to get \"\n",
      "Target data:  \"it gets harder each time he leaves. I haven't been able to get a\"\n",
      "Step    0\n",
      " input: 1 (' ')\n",
      " expected_output: 71 ('i')\n",
      "Step    1\n",
      " input: 71 ('i')\n",
      " expected_output: 82 ('t')\n",
      "Step    2\n",
      " input: 82 ('t')\n",
      " expected_output: 1 (' ')\n",
      "Step    3\n",
      " input: 1 (' ')\n",
      " expected_output: 69 ('g')\n",
      "Step    4\n",
      " input: 69 ('g')\n",
      " expected_output: 67 ('e')\n"
     ]
    }
   ],
   "source": [
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)\n",
    "\n",
    "for input_example, target_example in dataset.take(1):\n",
    "    print('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
    "    print('Target data: ', repr(''.join(idx2char[target_example.numpy()])))\n",
    "    \n",
    "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
    "    print('Step {:4d}'.format(i))\n",
    "    print(' input: {} ({:s})'.format(input_idx, repr(idx2char[input_idx])))\n",
    "    print(' expected_output: {} ({:s})'.format(target_idx, repr(idx2char[target_idx])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((64, 64), (64, 64)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "# Buffer size to shuffle the dataset in memory\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(batch_size, vocab, embedding_dim=256, rnn_units=512):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(len(vocab), \n",
    "                                  embedding_dim, \n",
    "                                  batch_input_shape=[batch_size, None]), \n",
    "        \n",
    "        tf.keras.layers.LSTM(rnn_units, \n",
    "                             return_sequences=True, \n",
    "                             stateful=True, \n",
    "                             recurrent_initializer='glorot_uniform'),\n",
    "        \n",
    "        tf.keras.layers.Dense(len(vocab))\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = get_model(BATCH_SIZE, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 99,  58, 153,  35,  38, 177,  80,   7,  13, 115,  58,  89, 177,\n",
       "       109, 184,  34, 172, 126,   5, 144, 156, 164,  48,   8, 177, 147,\n",
       "        60,  31,  24, 192,  43,  24,  73,  14,  68,  76, 105,  89,  70,\n",
       "       190,  31,   2,  74, 147, 124, 158, 153,  18, 137,  87,   9,  31,\n",
       "       103, 114, 152, 172, 141, 113, 159, 115,  89,  24,   3, 111])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "\n",
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices, axis=1).numpy()\n",
    "sampled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: \n",
      " \"cent! lol jk I'll accept you as a sister =P what else are you? b\"\n",
      "\n",
      "Next char prediction: \n",
      " \"\\x86\\\\ÂEHàr'-\\x99\\\\{à\\x93çDØ¥%·ÅÍR(àº^A8ïM8k.fn\\x8c{híA!lº£ÇÂ2°y)A\\x8a\\x98¿Ø´\\x97È\\x99{8#\\x95\"\n"
     ]
    }
   ],
   "source": [
    "print('Input: \\n', repr(''.join(idx2char[input_example_batch[0]])))\n",
    "print()\n",
    "print('Next char prediction: \\n', repr(''.join(idx2char[sampled_indices])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 1\n",
    "\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt_{epoch}')\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix, save_weights_only=True)\n",
    "\n",
    "model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))\n",
    "\n",
    "model.fit(dataset, \n",
    "          epochs=EPOCHS,\n",
    "          callbacks=[checkpoint_callback],\n",
    "          verbose=2\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string):\n",
    "    \n",
    "    # Number of characters to generate\n",
    "    num_generate = 1000\n",
    "    \n",
    "    # Coverting the start_string to numbers (vectorizing)\n",
    "    input_eval = [char2idx[s] for s in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "    \n",
    "    text_generated = []\n",
    "    # Low temperature --> more predictable text\n",
    "    # High temperature --> more surprising text\n",
    "    temperature = 1.0\n",
    "    \n",
    "    model.reset_states()\n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "        \n",
    "        # Using a categorical distribution to predict the char returned by the model\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
    "        \n",
    "        # We pass the predicted char to the model along with the previous hidden state\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "        \n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "        \n",
    "    return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generating_model = get_model(1, vocab)\n",
    "generating_model.load_weights(tf.train.latest_checkpoint(checkpoint_dir = './training_checkpoints'))\n",
    "generating_model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate_text(generating_model, start_string=u'Well, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
